## Problema de optmización de una función 
**Profesora**: Dra. Yenny Villuendas Rey <br>

<br>

### **1. Enuncie las ventajas y desventajas de la Ascensión de Colinas** 

**Ventajas:**
* <div class=text-justify>Los métodos de ascensión de colinas nos permiten encontrar soluciones aceptables para problemas complejos que no pueden ser resueltos (o no fácilmente) por algoritmos tradicionales.</div>
* <div class=text-justify>Tiene la posibilidad de encontrar las mejores soluciones en un espacio razonable de estados, a un costo en tiempo relativamente bajo dependiendo de la técnica de ascensión utilizada.</div>
* <div class=text-justify>Toma ventajas en donde los algoritmos deterministas no tienen control del camino a tomar, ya que las heurísticas le proveen de la capacidad de decisión sobre el mejor rumbo a tomar, mientras que los algoritmos tradicionales, tienen que realizar una búsqueda completa o encontrar una gorma Greedy de resolver dicho problema, junto con su respectiva demostración.</div>
* <div class=text-justify>Pueden utilizar diferentes técnicas para mejorar las posibles soluciones en zonas donde la ascensión de colinas puede verse detenida.</div>
* <div class=text-justify> Proporciona múltiples ventajas contra los algoritmos tradicionales de búsqueda ciega como lo puede ser la optimización en tiempo, o la optimización en memoria. </div>

**Desventajas:**
* <div class=text-justify>Necesita de una función generadora de estados que permita moverse alrededor de los estados vecinos para lograr un correcto ascenso, lo cual puede llegar a ser complicado dado el origen del problema.</div>
* <div class=text-justify>Se necesita de una heurística evaluadora que pondere el valor de un estado respecto a un estado deseado, de modo tal que un gran peso para la efectividad de la ascensión de colinas termina siendo la heurística.</div>
* <div class=text-justify>No tiene un buen balance entre el costo en tiempo contra el resultado obtenido, ya que, para obtener un resultado correcto, surge la necesidad de explorar más estados vecinos al mejor estado actual, lo cual lo hará más tardado para encontrar soluciones totalmente correctas.</div>
* <div class=text-justify>Tiene problemas en zonas donde la función sobre la que se evalúa la solución no cambia su valor con los estados vecinos, y es necesario aplicar diferentes técnicas más allá del propio algoritmo para superarlas.</div>

<br>

### **2. Detalle el pseudocódigo del algoritmo Random mutation hill-climbing (RMHC)**

**Algoritmo Random mutation hill-climbing (RMHC):**

1. <div class=text-justify>Elegir una cadena de bits aleatoriamente. Llamar a esta cadena la “mejor evaluada”.</div>
2. <div class=text-justify>Elegir una posición de la cadena de manera aleatoria y mutar (negar) el bit en esa posición.</div>
3. <div class=text-justify>Si la cadena mutada está mejor (o igualmente) evaluada que la cadena original (por una función objetivo), entonces llamamos a la cadena mutada la cadena “mejor evaluada”.</div>
4. Ir al paso 2.
5. <div class=text-justify>Cuando la función objetivo se haya evaluada un número determinado de veces, regresar la cadena “mejor evaluada”.</div>

**Pseudocódigo del algoritmo Random mutation hill-climbing (RMHC):**

En el primer paso se elige aleatoriamente una cadena de bits:
$$
\begin{equation}
\bar{x}\in \left\lbrace 0,1 \right\rbrace^n,
\end{equation} y a esta cadena la llamamos la \`\`mejor evaluada'' (best), es decir
$$ \text{best}\ \gets\bar{x}.$$

<br>

Después, se elige aleatoriamente una componente $$$x_\ell$ de $\bar{x}$$ y se define la cadena mutada $\bar{y}$ como:
$$\begin{align}
y_i&=x_i \:\forall  i\neq\ell, \\
y_{\ell}&= \neg x_{\ell}.
\end{align}
$$
Sea $f$ la función objetivo (la altura de la colina):

$$\textbf{If} \quad f(\bar{y})\geq f(\bar{x}) \quad \textbf{then} \quad \text{best} \gets\bar{y}.$$

Por último, la función objetivo se evalúa un número determinado de veces $k$ antes de regresar el valor de best. De manera que el pseudocódigo queda como:

$
\begin{align}
&\textbf{Input: } \bar{x}, \: f, \: k
\\
&\textbf{Output: } \text{best}
\\
&best \gets \bar{x}
\\
&\textbf{For } i=1,\dots,k \textbf{ do}
\\
&| \quad \text{Elegir aleatoriamente una componente } x_\ell \text{ de } \bar{x}.
\\
&| \quad \text{Definir } \bar{y} \text{ tal que } y_i=x_i \: \forall i\neq\ell, \: y_\ell=\lnot\ x_\ell.
\\
&| \quad \textbf{If } f(\bar{y})\geq f(\bar{x}) \textbf{ then }
\\
&| \quad | \quad \text{best} \gets\bar{y}.
\\
&| \quad \textbf{end}
\\
& \textbf{end}
\\
& \textbf{return } \text{best}
\end{align}
$
<br>


### **3. Realice la modelación matemática necesaria para la solución, mediante RMHC, del siguiente problema de optimización**

Obtención de mínimos de la función $f(x)=\sum_{i=1}^{D}{x_i^2}$, $-10\leq x_i \leq 10$

### Modelo Matemático ###
Definimos el problema de optimización como una $x = [x_1, \dots , x_i, \dots, x_D]$ tal que $1\leq i \leq D$ donde $-10 \leq x_i \leq 10$, por lo que podemos definir el espacio de soluciones como:

$$S = \{ x \in \mathbb{R}^D |  -10 \leq x_i \leq 10 \}$$

Teniendo como función objetivo a minimzar:

$$f(x)=\sum_{i=1}^{D}{x_i^2}$$

Y el operador de mutación $\hat{M}$ como:
$$ \hat{M}(x) = y$$

$$ \hat{M} : \mathbb{R}^D \to \mathbb{R}^D$$

donde

$$(\exists ! i \; | \;  x_i \neq y_i \;) \; \text{ y } \; y_i \sim \text{Uniforme}(-10, 10)$$

Por lo que tendremos como estado inicial, un estado aleatorio $a$ tal que $a \in S$, y el estado objetivo será un estado $y$ tal que $f(y) = min(f(z))$ para todo $z \in S$ 



### **4. Proponga las estructuras de datos necesarias para la implementación de la solución propuesta, mediante RMHC, al problema anterior.**
Se utilizará un arreglo $state$ de tamaño $D$ para representar a $x$. 
<br>

### **5. Diseñe la interfaz de usuario para la solución del problema planteado.** 

```python=1
import random
import copy
```

Decidimos colocar una lamda llamada *printPretty* para evitar imprimir todos los decimales de un número.
```python=4
d = 0
printPretty = lambda number:round(number,2)
```

Formalmente, nuestra función objetivo $f(x)$ esta representada por la función:
```python=7
def f(x):
  total = 0
  for xi in x:
    total += (xi*xi)
  return total
```

```python=13
def getStateScore(state):
  return f(state)
```

```python=16
def isValidState(state):
  if len(state) != d:
    return False
  for number in state:
      if number < -10 or number > 10:
        return False
  return True
```

Para generar un estado aleatorio, creamos un arreglo de tamaño $D$, y llenamos cada valor $x_i$ con un valor aleatorio de una distribución uniforme entre -10 y 10
```python=24
def randomState():
  random.seed(a=None)
  state = [0]*d
  while True:
    for i in range(0, d):
      state[i] = random.uniform(-10,10)
    if isValidState(state):
      break
  return state
```

Para la mutación, elegimos una posición aleatoria entre $0$ y $D-1$, para sustituir su valor con un valor aletorio en una distribución uniforme entre -10 y 10
```python=34
def mutateState(state):
  while True:
    state_copy = copy.copy(state)
    random_position = random.randint(0, len(state)-1)
    state_copy[random_position] = random.uniform(-10,10)
    if isValidState(state_copy) and state != state_copy:
      break
  return state_copy
```

El algoritmo de rmhc, se mantiene igual que como lo ha hecho en las practicas anteriores.
```python=43
def rmhc(max_iterations):
  state = randomState()
  state_score = getStateScore(state)
  print("Starting state ->", state)

  for i in range(0, max_iterations):
    mutation = mutateState(copy.copy(state))
    mutation_score = getStateScore(mutation)
    print("#", i, " ", round(state_score,2), "-> ",
        [printPretty(x) for x in state], "\tM ",
        round(mutation_score,2), " ->",
        [printPretty(x) for x in mutation], end='')

    if mutation_score < state_score:
      print(" !better", end='')
      state = mutation
      state_score = mutation_score
      
    print()
  return state
```
**Input:**
```python=65
#Datos de entrada
d = 10
```

```python=68
solutionPath = rmhc(100) #Best Path = 63
map(printPretty, solutionPath)
print(solutionPath)
```

Finalmente, en la salida de nuestro programa, podemos observar como el algoritmo muestra una tendencia a minimizar el valor de nuestra función $f(x)$ llegando hasta el valor de 14, sin embargo no tan cercano a 0 debido a las bajas probabilidades de nuestra distribución uniforme de coincidir con mutaciones que tiendan a 0. 

**Output:**
```text
Starting state -> [-3.866618788302503, 1.279605688288413, 8.526137888839283, 7.025401499384557, -9.853872775190737, 7.704303442401898, -1.477139451048222, 9.78052657281047, -3.0655838079322395, 0.9455126375494576]
# 0   403.23 ->  [-3.87, 1.28, 8.53, 7.03, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95] 	M  442.39  -> [-3.87, 1.28, 8.53, 7.03, -9.85, 9.93, -1.48, 9.78, -3.07, 0.95]
# 1   403.23 ->  [-3.87, 1.28, 8.53, 7.03, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95] 	M  384.75  -> [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95] !better
# 2   384.75 ->  [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95] 	M  424.41  -> [-3.87, -6.43, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95]
# 3   384.75 ->  [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, 0.95] 	M  384.04  -> [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 4   384.04 ->  [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  402.74  -> [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -5.3, -0.43]
# 5   384.04 ->  [-3.87, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  382.44  -> [3.65, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 6   382.44 ->  [3.65, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  403.02  -> [3.65, 1.28, -9.66, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43]
# 7   382.44 ->  [3.65, 1.28, 8.53, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  362.03  -> [3.65, 1.28, 7.23, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 8   362.03 ->  [3.65, 1.28, 7.23, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  381.85  -> [3.65, 1.28, 7.23, -5.56, -9.85, 7.7, -1.48, 9.78, -5.41, -0.43]
# 9   362.03 ->  [3.65, 1.28, 7.23, -5.56, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  332.86  -> [3.65, 1.28, 7.23, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 10   332.86 ->  [3.65, 1.28, 7.23, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  315.15  -> [3.65, 1.28, 5.88, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 11   315.15 ->  [3.65, 1.28, 5.88, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  389.12  -> [3.65, 1.28, 5.88, -1.31, -9.85, 7.7, 8.73, 9.78, -3.07, -0.43]
# 12   315.15 ->  [3.65, 1.28, 5.88, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  359.62  -> [3.65, 1.28, -8.89, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43]
# 13   315.15 ->  [3.65, 1.28, 5.88, -1.31, -9.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  218.77  -> [3.65, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 14   218.77 ->  [3.65, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  231.49  -> [3.65, -3.79, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43]
# 15   218.77 ->  [3.65, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  218.79  -> [3.65, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, 3.07, -0.43]
# 16   218.77 ->  [3.65, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  217.14  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] !better
# 17   217.14 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  234.16  -> [3.42, 1.28, 5.88, -1.31, -4.21, 7.7, -1.48, 9.78, -3.07, -0.43]
# 18   217.14 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  222.17  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, 2.69, 9.78, -3.07, -0.43]
# 19   217.14 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  220.08  -> [-3.83, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43]
# 20   217.14 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  224.29  -> [3.42, 1.28, 5.88, -1.31, -2.81, 7.7, -1.48, 9.78, -3.07, -0.43]
# 21   217.14 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.78, -3.07, -0.43] 	M  133.97  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] !better
# 22   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  136.08  -> [3.72, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43]
# 23   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  165.52  -> [6.58, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43]
# 24   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  149.97  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -4.26, -3.53, -3.07, -0.43]
# 25   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  161.16  -> [3.42, 1.28, 5.88, -1.31, -0.85, -9.3, -1.48, -3.53, -3.07, -0.43]
# 26   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  196.79  -> [3.42, 1.28, -9.87, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43]
# 27   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  134.1  -> [3.42, 1.28, 5.88, -1.31, 0.93, 7.7, -1.48, -3.53, -3.07, -0.43]
# 28   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  224.2  -> [3.42, 1.28, 5.88, -1.31, -9.54, 7.7, -1.48, -3.53, -3.07, -0.43]
# 29   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  212.75  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, 9.55, -3.07, -0.43]
# 30   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  147.29  -> [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -3.67]
# 31   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  221.89  -> [-9.98, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43]
# 32   133.97 ->  [3.42, 1.28, 5.88, -1.31, -0.85, 7.7, -1.48, -3.53, -3.07, -0.43] 	M  116.21  -> [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] !better
# 33   116.21 ->  [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  192.42  -> [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -9.25, -0.43]
# 34   116.21 ->  [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  171.8  -> [3.42, 1.28, 5.88, -1.31, 7.5, -6.45, -1.48, -3.53, -3.07, -0.43]
# 35   116.21 ->  [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  205.16  -> [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -9.55, -3.53, -3.07, -0.43]
# 36   116.21 ->  [3.42, 1.28, 5.88, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  103.99  -> [3.42, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] !better
# 37   103.99 ->  [3.42, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  94.88  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] !better
# 38   94.88 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  121.31  -> [-1.61, 1.28, -4.73, -1.31, -5.21, -6.45, -1.48, -3.53, -3.07, -0.43]
# 39   94.88 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  98.26  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, 2.36, -3.53, -3.07, -0.43]
# 40   94.88 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, -3.07, -0.43] 	M  86.12  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] !better
# 41   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  124.95  -> [-1.61, 6.36, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 42   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  88.72  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, 1.67]
# 43   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  153.13  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, 8.32, -3.53, 0.8, -0.43]
# 44   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  141.39  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, 7.45]
# 45   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  152.1  -> [-1.61, 1.28, -4.73, -8.23, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 46   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  153.07  -> [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -8.31, -3.53, 0.8, -0.43]
# 47   86.12 ->  [-1.61, 1.28, -4.73, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  76.01  -> [-1.61, 1.28, 3.5, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] !better
# 48   76.01 ->  [-1.61, 1.28, 3.5, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  113.37  -> [-1.61, 1.28, 3.5, -1.31, -0.85, 8.89, -1.48, -3.53, 0.8, -0.43]
# 49   76.01 ->  [-1.61, 1.28, 3.5, -1.31, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  74.68  -> [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] !better
# 50   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  99.65  -> [-1.61, 1.28, 3.5, -0.61, 5.07, -6.45, -1.48, -3.53, 0.8, -0.43]
# 51   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  74.91  -> [-1.68, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 52   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  136.59  -> [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 7.91, -0.43]
# 53   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  80.63  -> [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, -2.57, -0.43]
# 54   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  100.05  -> [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, -5.1, -0.43]
# 55   74.68 ->  [-1.61, 1.28, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  73.18  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] !better
# 56   73.18 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  153.93  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -9.11, -3.53, 0.8, -0.43]
# 57   73.18 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  85.85  -> [-3.91, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 58   73.18 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  103.46  -> [-1.61, -0.37, 3.5, 5.54, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 59   73.18 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  89.1  -> [-1.61, -0.37, 3.5, -4.04, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43]
# 60   73.18 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -6.45, -1.48, -3.53, 0.8, -0.43] 	M  33.65  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] !better
# 61   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  105.03  -> [8.6, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43]
# 62   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  95.98  -> [-8.06, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43]
# 63   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  36.25  -> [-1.61, -0.37, 3.5, 1.72, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43]
# 64   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  38.97  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -2.74, -3.53, 0.8, -0.43]
# 65   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  102.39  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -8.41, -1.48, -3.53, 0.8, -0.43]
# 66   33.65 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -1.48, -3.53, 0.8, -0.43] 	M  31.97  -> [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -0.71, -3.53, 0.8, -0.43] !better
# 67   31.97 ->  [-1.61, -0.37, 3.5, -0.61, -0.85, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  31.32  -> [-1.61, -0.37, 3.5, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] !better
# 68   31.32 ->  [-1.61, -0.37, 3.5, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  84.22  -> [-1.61, -0.37, 3.5, -0.61, 0.28, -1.44, -0.71, -3.53, 7.32, -0.43]
# 69   31.32 ->  [-1.61, -0.37, 3.5, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  19.57  -> [-1.61, -0.37, 0.71, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] !better
# 70   19.57 ->  [-1.61, -0.37, 0.71, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  108.24  -> [-1.61, -0.37, 0.71, -0.61, -9.42, -1.44, -0.71, -3.53, 0.8, -0.43]
# 71   19.57 ->  [-1.61, -0.37, 0.71, -0.61, 0.28, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  19.54  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] !better
# 72   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  102.76  -> [-1.61, -0.37, 9.15, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 73   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  57.73  -> [-1.61, 6.19, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 74   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  19.87  -> [-1.71, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 75   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  25.44  -> [-1.61, -0.37, 0.71, -2.51, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 76   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  44.45  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -5.05, -0.43]
# 77   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  108.02  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, 9.43, -3.53, 0.8, -0.43]
# 78   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  26.23  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -2.96, -0.71, -3.53, 0.8, -0.43]
# 79   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  66.19  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 6.88, -0.43]
# 80   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  77.51  -> [-1.61, -0.37, 0.71, 7.64, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 81   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  49.36  -> [-1.61, -0.37, 0.71, -0.61, -5.47, -1.44, -0.71, -3.53, 0.8, -0.43]
# 82   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  27.56  -> [-1.61, -0.37, -2.92, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43]
# 83   19.54 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, 0.8, -0.43] 	M  18.94  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] !better
# 84   18.94 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] 	M  37.72  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, 5.59, -0.19, -0.43]
# 85   18.94 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] 	M  20.79  -> [-1.61, -0.37, 0.71, -1.49, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43]
# 86   18.94 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] 	M  23.68  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -2.22]
# 87   18.94 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] 	M  36.93  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -4.25, -0.43]
# 88   18.94 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -3.53, -0.19, -0.43] 	M  14.67  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] !better
# 89   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  67.53  -> [-1.61, -0.37, 0.71, -7.3, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43]
# 90   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  43.59  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -6.09, -0.19, -0.43]
# 91   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  67.49  -> [-1.61, -0.37, 0.71, 7.29, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43]
# 92   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  47.26  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, 5.71, -0.43]
# 93   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  53.63  -> [-1.61, -0.37, 0.71, -0.61, 6.25, -1.44, -0.71, -2.87, -0.19, -0.43]
# 94   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  29.12  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -4.06, -0.71, -2.87, -0.19, -0.43]
# 95   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  15.67  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, 1.23, -2.87, -0.19, -0.43]
# 96   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  17.73  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -2.26, -0.71, -2.87, -0.19, -0.43]
# 97   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  36.81  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, 4.76, -2.87, -0.19, -0.43]
# 98   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  98.91  -> [-1.61, -0.37, -9.21, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43]
# 99   14.67 ->  [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, -0.19, -0.43] 	M  63.33  -> [-1.61, -0.37, 0.71, -0.61, -0.21, -1.44, -0.71, -2.87, 6.98, -0.43]
[-1.6133880956293645, -0.3749443851104708, 0.7068788503058734, -0.6102759980834094, -0.20579793500690613, -1.4375451945075426, -0.708393210566566, -2.8679259431649324, -0.19237585921451306, -0.4307888144862382]```
